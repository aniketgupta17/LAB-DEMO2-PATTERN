/home/Student/s4824063/miniconda3/envs/conda-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Random Seed:  999
Generator(
  (main): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
Discriminator(
  (main): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (12): Sigmoid()
  )
)
Starting Training Loop...
[0/5][0/1583]	Loss_D: 1.7237	Loss_G: 5.2575	D(x): 0.5539	D(G(z)): 0.5840 / 0.0080
[0/5][50/1583]	Loss_D: 0.0518	Loss_G: 31.9846	D(x): 0.9800	D(G(z)): 0.0000 / 0.0000
[0/5][100/1583]	Loss_D: 0.2973	Loss_G: 6.9254	D(x): 0.9564	D(G(z)): 0.1624 / 0.0024
[0/5][150/1583]	Loss_D: 0.2238	Loss_G: 3.6194	D(x): 0.9188	D(G(z)): 0.1008 / 0.0494
[0/5][200/1583]	Loss_D: 0.5792	Loss_G: 4.4420	D(x): 0.6887	D(G(z)): 0.0557 / 0.0240
[0/5][250/1583]	Loss_D: 0.4037	Loss_G: 4.4712	D(x): 0.8179	D(G(z)): 0.1132 / 0.0181
[0/5][300/1583]	Loss_D: 0.4686	Loss_G: 4.6205	D(x): 0.8794	D(G(z)): 0.2333 / 0.0221
[0/5][350/1583]	Loss_D: 0.6567	Loss_G: 4.8981	D(x): 0.8184	D(G(z)): 0.2909 / 0.0132
[0/5][400/1583]	Loss_D: 0.5571	Loss_G: 3.4157	D(x): 0.7885	D(G(z)): 0.2017 / 0.0544
[0/5][450/1583]	Loss_D: 0.4241	Loss_G: 4.8392	D(x): 0.8230	D(G(z)): 0.1207 / 0.0157
[0/5][500/1583]	Loss_D: 0.4940	Loss_G: 7.3094	D(x): 0.9796	D(G(z)): 0.3197 / 0.0020
[0/5][550/1583]	Loss_D: 0.4776	Loss_G: 4.0252	D(x): 0.7237	D(G(z)): 0.0352 / 0.0318
[0/5][600/1583]	Loss_D: 1.2307	Loss_G: 3.0707	D(x): 0.4146	D(G(z)): 0.0048 / 0.1082
[0/5][650/1583]	Loss_D: 0.8323	Loss_G: 4.0192	D(x): 0.5536	D(G(z)): 0.0182 / 0.0433
[0/5][700/1583]	Loss_D: 0.3955	Loss_G: 4.7417	D(x): 0.8700	D(G(z)): 0.1522 / 0.0166
[0/5][750/1583]	Loss_D: 1.2706	Loss_G: 1.5227	D(x): 0.4882	D(G(z)): 0.0180 / 0.3313
[0/5][800/1583]	Loss_D: 0.5358	Loss_G: 5.2010	D(x): 0.8515	D(G(z)): 0.2575 / 0.0093
[0/5][850/1583]	Loss_D: 0.3984	Loss_G: 4.1563	D(x): 0.7613	D(G(z)): 0.0308 / 0.0271
[0/5][900/1583]	Loss_D: 0.9509	Loss_G: 7.2113	D(x): 0.9212	D(G(z)): 0.5136 / 0.0016
[0/5][950/1583]	Loss_D: 2.3919	Loss_G: 1.6489	D(x): 0.2059	D(G(z)): 0.0080 / 0.3249
[0/5][1000/1583]	Loss_D: 0.4569	Loss_G: 5.0902	D(x): 0.8558	D(G(z)): 0.1786 / 0.0145
[0/5][1050/1583]	Loss_D: 0.9938	Loss_G: 3.3215	D(x): 0.4832	D(G(z)): 0.0102 / 0.0680
[0/5][1100/1583]	Loss_D: 0.8874	Loss_G: 6.4585	D(x): 0.9439	D(G(z)): 0.4699 / 0.0054
[0/5][1150/1583]	Loss_D: 0.7957	Loss_G: 1.4497	D(x): 0.5837	D(G(z)): 0.0363 / 0.3651
[0/5][1200/1583]	Loss_D: 0.3401	Loss_G: 4.5840	D(x): 0.8764	D(G(z)): 0.1640 / 0.0162
[0/5][1250/1583]	Loss_D: 0.5068	Loss_G: 3.1706	D(x): 0.6966	D(G(z)): 0.0657 / 0.0677
[0/5][1300/1583]	Loss_D: 0.3525	Loss_G: 3.2972	D(x): 0.8384	D(G(z)): 0.1328 / 0.0587
[0/5][1350/1583]	Loss_D: 0.4466	Loss_G: 2.7232	D(x): 0.7458	D(G(z)): 0.0831 / 0.1015
[0/5][1400/1583]	Loss_D: 0.2633	Loss_G: 3.4764	D(x): 0.9187	D(G(z)): 0.1469 / 0.0495
[0/5][1450/1583]	Loss_D: 0.6861	Loss_G: 4.9692	D(x): 0.9097	D(G(z)): 0.3790 / 0.0130
[0/5][1500/1583]	Loss_D: 0.8994	Loss_G: 7.3156	D(x): 0.9611	D(G(z)): 0.5034 / 0.0018
[0/5][1550/1583]	Loss_D: 0.4752	Loss_G: 3.1236	D(x): 0.8185	D(G(z)): 0.1987 / 0.0709
[1/5][0/1583]	Loss_D: 0.5505	Loss_G: 4.4666	D(x): 0.8792	D(G(z)): 0.3010 / 0.0192
[1/5][50/1583]	Loss_D: 1.6910	Loss_G: 3.1478	D(x): 0.3069	D(G(z)): 0.0015 / 0.0983
[1/5][100/1583]	Loss_D: 0.3511	Loss_G: 3.4045	D(x): 0.8446	D(G(z)): 0.1195 / 0.0492
[1/5][150/1583]	Loss_D: 0.8350	Loss_G: 2.1206	D(x): 0.5279	D(G(z)): 0.0169 / 0.1742
[1/5][200/1583]	Loss_D: 0.5951	Loss_G: 1.6160	D(x): 0.6476	D(G(z)): 0.0568 / 0.2658
[1/5][250/1583]	Loss_D: 1.0930	Loss_G: 0.7592	D(x): 0.4607	D(G(z)): 0.0120 / 0.5563
[1/5][300/1583]	Loss_D: 0.4728	Loss_G: 2.8461	D(x): 0.7948	D(G(z)): 0.1775 / 0.0793
[1/5][350/1583]	Loss_D: 0.4499	Loss_G: 2.8100	D(x): 0.7349	D(G(z)): 0.0565 / 0.1002
[1/5][400/1583]	Loss_D: 0.4486	Loss_G: 2.5164	D(x): 0.7408	D(G(z)): 0.0924 / 0.1187
[1/5][450/1583]	Loss_D: 1.7879	Loss_G: 9.0050	D(x): 0.9717	D(G(z)): 0.7384 / 0.0007
[1/5][500/1583]	Loss_D: 0.9697	Loss_G: 5.2141	D(x): 0.9595	D(G(z)): 0.5357 / 0.0118
[1/5][550/1583]	Loss_D: 0.3812	Loss_G: 2.8858	D(x): 0.8867	D(G(z)): 0.1995 / 0.0839
[1/5][600/1583]	Loss_D: 0.8201	Loss_G: 1.2569	D(x): 0.5170	D(G(z)): 0.0168 / 0.3569
[1/5][650/1583]	Loss_D: 0.5476	Loss_G: 2.5065	D(x): 0.7054	D(G(z)): 0.1209 / 0.1209
[1/5][700/1583]	Loss_D: 0.3565	Loss_G: 2.6764	D(x): 0.8343	D(G(z)): 0.1289 / 0.0957
[1/5][750/1583]	Loss_D: 0.8768	Loss_G: 1.3273	D(x): 0.4997	D(G(z)): 0.0333 / 0.3189
[1/5][800/1583]	Loss_D: 0.5952	Loss_G: 3.0492	D(x): 0.6177	D(G(z)): 0.0154 / 0.0744
[1/5][850/1583]	Loss_D: 1.1927	Loss_G: 6.0811	D(x): 0.9699	D(G(z)): 0.6135 / 0.0045
[1/5][900/1583]	Loss_D: 0.7824	Loss_G: 1.9949	D(x): 0.5575	D(G(z)): 0.0422 / 0.1866
[1/5][950/1583]	Loss_D: 0.5433	Loss_G: 2.0038	D(x): 0.6923	D(G(z)): 0.0866 / 0.1815
[1/5][1000/1583]	Loss_D: 0.6012	Loss_G: 2.6938	D(x): 0.8111	D(G(z)): 0.2885 / 0.0889
[1/5][1050/1583]	Loss_D: 0.9002	Loss_G: 1.2962	D(x): 0.6318	D(G(z)): 0.2452 / 0.3382
[1/5][1100/1583]	Loss_D: 0.6942	Loss_G: 3.9754	D(x): 0.8729	D(G(z)): 0.3714 / 0.0299
[1/5][1150/1583]	Loss_D: 0.9089	Loss_G: 6.0231	D(x): 0.9124	D(G(z)): 0.5039 / 0.0044
[1/5][1200/1583]	Loss_D: 0.3358	Loss_G: 3.6626	D(x): 0.8994	D(G(z)): 0.1885 / 0.0372
[1/5][1250/1583]	Loss_D: 1.0020	Loss_G: 4.2911	D(x): 0.9011	D(G(z)): 0.5277 / 0.0242
[1/5][1300/1583]	Loss_D: 0.5231	Loss_G: 2.4883	D(x): 0.7848	D(G(z)): 0.2060 / 0.1080
[1/5][1350/1583]	Loss_D: 0.5621	Loss_G: 2.5401	D(x): 0.7510	D(G(z)): 0.1893 / 0.1029
[1/5][1400/1583]	Loss_D: 0.7162	Loss_G: 1.0433	D(x): 0.5797	D(G(z)): 0.0668 / 0.4196
[1/5][1450/1583]	Loss_D: 0.4247	Loss_G: 2.3947	D(x): 0.7769	D(G(z)): 0.1297 / 0.1165
[1/5][1500/1583]	Loss_D: 0.4410	Loss_G: 2.7294	D(x): 0.8619	D(G(z)): 0.2159 / 0.0885
[1/5][1550/1583]	Loss_D: 0.6413	Loss_G: 3.2817	D(x): 0.8644	D(G(z)): 0.3483 / 0.0522
[2/5][0/1583]	Loss_D: 0.5574	Loss_G: 1.7775	D(x): 0.7580	D(G(z)): 0.2012 / 0.2051
[2/5][50/1583]	Loss_D: 0.8908	Loss_G: 1.5840	D(x): 0.5297	D(G(z)): 0.0604 / 0.2695
[2/5][100/1583]	Loss_D: 1.0339	Loss_G: 5.1728	D(x): 0.9335	D(G(z)): 0.5706 / 0.0095
[2/5][150/1583]	Loss_D: 0.6021	Loss_G: 3.6996	D(x): 0.9065	D(G(z)): 0.3532 / 0.0376
[2/5][200/1583]	Loss_D: 0.4134	Loss_G: 1.9926	D(x): 0.8077	D(G(z)): 0.1552 / 0.1657
[2/5][250/1583]	Loss_D: 0.5258	Loss_G: 2.7057	D(x): 0.8074	D(G(z)): 0.2363 / 0.0910
[2/5][300/1583]	Loss_D: 0.5504	Loss_G: 2.7752	D(x): 0.8263	D(G(z)): 0.2704 / 0.0794
[2/5][350/1583]	Loss_D: 0.8606	Loss_G: 1.5446	D(x): 0.5607	D(G(z)): 0.1380 / 0.2701
[2/5][400/1583]	Loss_D: 0.6978	Loss_G: 4.1154	D(x): 0.9138	D(G(z)): 0.4173 / 0.0250
[2/5][450/1583]	Loss_D: 0.6504	Loss_G: 1.5924	D(x): 0.6158	D(G(z)): 0.1029 / 0.2544
[2/5][500/1583]	Loss_D: 1.6220	Loss_G: 1.5797	D(x): 0.2683	D(G(z)): 0.0135 / 0.2729
[2/5][550/1583]	Loss_D: 0.4163	Loss_G: 2.6908	D(x): 0.8312	D(G(z)): 0.1861 / 0.0841
[2/5][600/1583]	Loss_D: 1.0211	Loss_G: 4.7574	D(x): 0.9460	D(G(z)): 0.5800 / 0.0120
[2/5][650/1583]	Loss_D: 0.7147	Loss_G: 3.2562	D(x): 0.8343	D(G(z)): 0.3758 / 0.0488
[2/5][700/1583]	Loss_D: 0.6116	Loss_G: 1.2454	D(x): 0.6178	D(G(z)): 0.0706 / 0.3316
[2/5][750/1583]	Loss_D: 0.9578	Loss_G: 4.1596	D(x): 0.8965	D(G(z)): 0.5244 / 0.0217
[2/5][800/1583]	Loss_D: 0.5613	Loss_G: 2.8261	D(x): 0.7937	D(G(z)): 0.2385 / 0.0777
[2/5][850/1583]	Loss_D: 0.4998	Loss_G: 2.3685	D(x): 0.7459	D(G(z)): 0.1416 / 0.1316
[2/5][900/1583]	Loss_D: 0.7549	Loss_G: 4.0747	D(x): 0.8659	D(G(z)): 0.4128 / 0.0232
[2/5][950/1583]	Loss_D: 0.6466	Loss_G: 1.7515	D(x): 0.6911	D(G(z)): 0.1892 / 0.2166
[2/5][1000/1583]	Loss_D: 0.9323	Loss_G: 3.6936	D(x): 0.9196	D(G(z)): 0.5130 / 0.0352
[2/5][1050/1583]	Loss_D: 1.0728	Loss_G: 0.9819	D(x): 0.4076	D(G(z)): 0.0137 / 0.4490
[2/5][1100/1583]	Loss_D: 0.9947	Loss_G: 3.8475	D(x): 0.9739	D(G(z)): 0.5601 / 0.0348
[2/5][1150/1583]	Loss_D: 1.0651	Loss_G: 4.1668	D(x): 0.9550	D(G(z)): 0.5869 / 0.0226
[2/5][1200/1583]	Loss_D: 0.6936	Loss_G: 4.1565	D(x): 0.8913	D(G(z)): 0.3922 / 0.0215
[2/5][1250/1583]	Loss_D: 0.3564	Loss_G: 3.4900	D(x): 0.8673	D(G(z)): 0.1634 / 0.0454
[2/5][1300/1583]	Loss_D: 0.5586	Loss_G: 2.1589	D(x): 0.7302	D(G(z)): 0.1820 / 0.1475
[2/5][1350/1583]	Loss_D: 0.5188	Loss_G: 2.7849	D(x): 0.9009	D(G(z)): 0.3040 / 0.0808
[2/5][1400/1583]	Loss_D: 0.5496	Loss_G: 2.7760	D(x): 0.8453	D(G(z)): 0.2863 / 0.0815
[2/5][1450/1583]	Loss_D: 0.5594	Loss_G: 2.8546	D(x): 0.8233	D(G(z)): 0.2692 / 0.0742
[2/5][1500/1583]	Loss_D: 0.5291	Loss_G: 2.6485	D(x): 0.8256	D(G(z)): 0.2513 / 0.0946
[2/5][1550/1583]	Loss_D: 0.9894	Loss_G: 1.0096	D(x): 0.4665	D(G(z)): 0.0806 / 0.4194
[3/5][0/1583]	Loss_D: 0.6827	Loss_G: 1.8131	D(x): 0.6027	D(G(z)): 0.0985 / 0.2215
[3/5][50/1583]	Loss_D: 0.6926	Loss_G: 2.2281	D(x): 0.7277	D(G(z)): 0.2531 / 0.1332
[3/5][100/1583]	Loss_D: 0.4807	Loss_G: 2.1485	D(x): 0.7928	D(G(z)): 0.1861 / 0.1501
[3/5][150/1583]	Loss_D: 0.8521	Loss_G: 3.7017	D(x): 0.9033	D(G(z)): 0.4868 / 0.0342
[3/5][200/1583]	Loss_D: 0.5334	Loss_G: 3.0204	D(x): 0.8345	D(G(z)): 0.2643 / 0.0668
[3/5][250/1583]	Loss_D: 0.7693	Loss_G: 3.3059	D(x): 0.8153	D(G(z)): 0.3806 / 0.0516
[3/5][300/1583]	Loss_D: 0.5355	Loss_G: 2.1012	D(x): 0.7646	D(G(z)): 0.2058 / 0.1451
[3/5][350/1583]	Loss_D: 0.4279	Loss_G: 2.6923	D(x): 0.7774	D(G(z)): 0.1368 / 0.0885
[3/5][400/1583]	Loss_D: 1.6464	Loss_G: 5.8990	D(x): 0.9671	D(G(z)): 0.7499 / 0.0045
[3/5][450/1583]	Loss_D: 0.5517	Loss_G: 2.9907	D(x): 0.8438	D(G(z)): 0.2878 / 0.0667
[3/5][500/1583]	Loss_D: 1.2249	Loss_G: 3.5254	D(x): 0.8532	D(G(z)): 0.5886 / 0.0470
[3/5][550/1583]	Loss_D: 0.7342	Loss_G: 3.3590	D(x): 0.8873	D(G(z)): 0.4122 / 0.0464
[3/5][600/1583]	Loss_D: 0.6035	Loss_G: 2.3320	D(x): 0.7806	D(G(z)): 0.2675 / 0.1247
[3/5][650/1583]	Loss_D: 0.5327	Loss_G: 2.1606	D(x): 0.8008	D(G(z)): 0.2250 / 0.1503
[3/5][700/1583]	Loss_D: 0.8172	Loss_G: 0.9469	D(x): 0.5286	D(G(z)): 0.0791 / 0.4337
[3/5][750/1583]	Loss_D: 0.6051	Loss_G: 2.6015	D(x): 0.8261	D(G(z)): 0.2992 / 0.0972
[3/5][800/1583]	Loss_D: 0.7977	Loss_G: 3.5317	D(x): 0.9150	D(G(z)): 0.4530 / 0.0426
[3/5][850/1583]	Loss_D: 0.8184	Loss_G: 4.2110	D(x): 0.8607	D(G(z)): 0.4351 / 0.0206
[3/5][900/1583]	Loss_D: 0.6035	Loss_G: 1.9894	D(x): 0.7023	D(G(z)): 0.1662 / 0.1617
[3/5][950/1583]	Loss_D: 0.9216	Loss_G: 3.8299	D(x): 0.9191	D(G(z)): 0.5144 / 0.0329
[3/5][1000/1583]	Loss_D: 0.8440	Loss_G: 4.4277	D(x): 0.8761	D(G(z)): 0.4690 / 0.0159
[3/5][1050/1583]	Loss_D: 0.5435	Loss_G: 3.2235	D(x): 0.8849	D(G(z)): 0.3170 / 0.0544
[3/5][1100/1583]	Loss_D: 0.4419	Loss_G: 2.3228	D(x): 0.7776	D(G(z)): 0.1457 / 0.1351
[3/5][1150/1583]	Loss_D: 1.0544	Loss_G: 1.1074	D(x): 0.4628	D(G(z)): 0.1188 / 0.4015
[3/5][1200/1583]	Loss_D: 1.0260	Loss_G: 3.5108	D(x): 0.8839	D(G(z)): 0.5362 / 0.0415
[3/5][1250/1583]	Loss_D: 1.0244	Loss_G: 4.7165	D(x): 0.8743	D(G(z)): 0.5291 / 0.0130
[3/5][1300/1583]	Loss_D: 2.1621	Loss_G: 0.9082	D(x): 0.1711	D(G(z)): 0.0183 / 0.4866
[3/5][1350/1583]	Loss_D: 0.6008	Loss_G: 2.4041	D(x): 0.6982	D(G(z)): 0.1650 / 0.1226
[3/5][1400/1583]	Loss_D: 0.5065	Loss_G: 2.6679	D(x): 0.8369	D(G(z)): 0.2534 / 0.0913
[3/5][1450/1583]	Loss_D: 0.6668	Loss_G: 4.0959	D(x): 0.8758	D(G(z)): 0.3684 / 0.0255
[3/5][1500/1583]	Loss_D: 0.6193	Loss_G: 2.7332	D(x): 0.7487	D(G(z)): 0.2405 / 0.0884
[3/5][1550/1583]	Loss_D: 1.0404	Loss_G: 0.9561	D(x): 0.4382	D(G(z)): 0.0558 / 0.4423
/home/Student/s4824063/miniconda3/envs/conda-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[4/5][0/1583]	Loss_D: 0.8396	Loss_G: 0.9423	D(x): 0.5290	D(G(z)): 0.1016 / 0.4293
[4/5][50/1583]	Loss_D: 0.7094	Loss_G: 2.0765	D(x): 0.7796	D(G(z)): 0.3030 / 0.1652
[4/5][100/1583]	Loss_D: 0.5471	Loss_G: 2.2764	D(x): 0.8010	D(G(z)): 0.2478 / 0.1251
[4/5][150/1583]	Loss_D: 0.3964	Loss_G: 3.0934	D(x): 0.7929	D(G(z)): 0.1289 / 0.0650
[4/5][200/1583]	Loss_D: 0.5524	Loss_G: 2.5184	D(x): 0.7206	D(G(z)): 0.1521 / 0.1129
[4/5][250/1583]	Loss_D: 0.4011	Loss_G: 2.6437	D(x): 0.7926	D(G(z)): 0.1320 / 0.0964
[4/5][300/1583]	Loss_D: 0.4589	Loss_G: 2.3464	D(x): 0.7351	D(G(z)): 0.1016 / 0.1200
[4/5][350/1583]	Loss_D: 1.2987	Loss_G: 0.3870	D(x): 0.3352	D(G(z)): 0.0250 / 0.7214
[4/5][400/1583]	Loss_D: 0.8726	Loss_G: 1.4367	D(x): 0.4993	D(G(z)): 0.0673 / 0.2899
[4/5][450/1583]	Loss_D: 0.5927	Loss_G: 1.9767	D(x): 0.6619	D(G(z)): 0.1074 / 0.1801
[4/5][500/1583]	Loss_D: 1.0867	Loss_G: 5.4327	D(x): 0.8959	D(G(z)): 0.5538 / 0.0094
[4/5][550/1583]	Loss_D: 0.5728	Loss_G: 2.1610	D(x): 0.7338	D(G(z)): 0.1865 / 0.1418
[4/5][600/1583]	Loss_D: 0.7456	Loss_G: 3.4736	D(x): 0.9214	D(G(z)): 0.4384 / 0.0428
[4/5][650/1583]	Loss_D: 2.1963	Loss_G: 0.5236	D(x): 0.1704	D(G(z)): 0.0279 / 0.6374
[4/5][700/1583]	Loss_D: 0.7094	Loss_G: 1.9572	D(x): 0.7206	D(G(z)): 0.2707 / 0.1653
[4/5][750/1583]	Loss_D: 0.7743	Loss_G: 3.0210	D(x): 0.8458	D(G(z)): 0.3929 / 0.0705
[4/5][800/1583]	Loss_D: 0.6057	Loss_G: 2.4667	D(x): 0.8701	D(G(z)): 0.3314 / 0.1126
[4/5][850/1583]	Loss_D: 1.0274	Loss_G: 1.2413	D(x): 0.4566	D(G(z)): 0.0786 / 0.3567
[4/5][900/1583]	Loss_D: 0.7821	Loss_G: 1.7078	D(x): 0.5380	D(G(z)): 0.0665 / 0.2283
[4/5][950/1583]	Loss_D: 0.6388	Loss_G: 1.5028	D(x): 0.6398	D(G(z)): 0.1202 / 0.2724
[4/5][1000/1583]	Loss_D: 0.6521	Loss_G: 4.2928	D(x): 0.9288	D(G(z)): 0.3966 / 0.0193
[4/5][1050/1583]	Loss_D: 0.8006	Loss_G: 1.1243	D(x): 0.5499	D(G(z)): 0.1071 / 0.3834
[4/5][1100/1583]	Loss_D: 0.5979	Loss_G: 2.7287	D(x): 0.8576	D(G(z)): 0.3167 / 0.0879
[4/5][1150/1583]	Loss_D: 0.7319	Loss_G: 1.7780	D(x): 0.6482	D(G(z)): 0.2059 / 0.2108
[4/5][1200/1583]	Loss_D: 2.7093	Loss_G: 6.1486	D(x): 0.9940	D(G(z)): 0.8973 / 0.0045
[4/5][1250/1583]	Loss_D: 0.7331	Loss_G: 2.7225	D(x): 0.8638	D(G(z)): 0.3999 / 0.0883
[4/5][1300/1583]	Loss_D: 0.5992	Loss_G: 3.3617	D(x): 0.8647	D(G(z)): 0.3242 / 0.0476
[4/5][1350/1583]	Loss_D: 0.6694	Loss_G: 4.4670	D(x): 0.9408	D(G(z)): 0.4204 / 0.0154
[4/5][1400/1583]	Loss_D: 0.9634	Loss_G: 5.0585	D(x): 0.9426	D(G(z)): 0.5414 / 0.0097
[4/5][1450/1583]	Loss_D: 0.4481	Loss_G: 2.6449	D(x): 0.8369	D(G(z)): 0.2143 / 0.0885
[4/5][1500/1583]	Loss_D: 2.2331	Loss_G: 0.4505	D(x): 0.1602	D(G(z)): 0.0231 / 0.6892
[4/5][1550/1583]	Loss_D: 1.0726	Loss_G: 4.0559	D(x): 0.9064	D(G(z)): 0.5617 / 0.0252
